{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVR3xBwBI-1x"
   },
   "source": [
    "# DATA5000 Assessment 1: Customer Retention and Revenue Optimization\n",
    "\n",
    "## Business Context\n",
    "\n",
    "You are a Business Analyst at a leading SaaS technology company. The executive team is concerned about customer churn and wants to develop a data-driven retention strategy.\n",
    "\n",
    "Your task is to analyze customer data, build predictive models, and evaluate the effectiveness of retention campaigns.\n",
    "\n",
    "## Important Instructions\n",
    "\n",
    "1. Run all cells in sequence from top to bottom\n",
    "2. Pay attention to the outputs and visualizations\n",
    "3. Take notes on key insights for your business report\n",
    "4. Save important charts by right-clicking and selecting \"Save image as\"\n",
    "5. This notebook should take 30-45 minutes to complete\n",
    "6. You are allowed to modify the code lines to fit with your narrative and you also are not required to use all visualizations (ONLY SELECT THOSE THAT ARE SUITABLE).\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset contains information about 15,000 SaaS customers including:\n",
    "- Customer demographics and firmographics\n",
    "- Engagement and usage metrics\n",
    "- Revenue and payment information\n",
    "- Churn status\n",
    "- Retention campaign participation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZvwZViqI-10"
   },
   "source": [
    "---\n",
    "\n",
    "# Section 0: Setup and Installation\n",
    "\n",
    "First, we will install all required libraries and import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.13 | packaged by conda-forge | (main, Jun  4 2025, 14:39:58) [MSC v.1943 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALL CELL -- Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install pytorch_forecasting==1.5.0 pytorch-lightning==2.6.0\n",
    "%pip install tensorboard==2.15.1\n",
    "%pip install numpy==1.26.4 scipy==1.11.4\n",
    "%pip install shap==0.48.0\n",
    "%pip install econml==0.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTANT – AFTER RUNNING THE INSTALL CELL, CLICK “Runtime → Restart Runtime” BEFORE CONTINUING, IF PROMPTED BY COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNus36YVI-11"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Machine Learning\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Explainable AI\n",
    "import shap\n",
    "\n",
    "# Causal Inference\n",
    "from econml.dr import DRLearner\n",
    "from econml.sklearn_extensions.linear_model import StatsModelsLinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "# Time Series Forecasting\n",
    "import torch\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "torch.serialization.add_safe_globals([GroupNormalizer])\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdOHwMmWI-11"
   },
   "source": [
    "---\n",
    "\n",
    "# Section 1A: Data Exploration and Preparation\n",
    "\n",
    "In this section, we will load the customer dataset and perform exploratory data analysis to understand customer behavior patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/clarkian-teachings/data5k/main/assessments/saas_customer_data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset loaded successfully\")\n",
    "print(f\"Number of customers: {len(df):,}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWwYUQu3I-12"
   },
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*60)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iN6yzH8pI-12"
   },
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"=\"*60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bzjk02gDI-12"
   },
   "source": [
    "## Key Business Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zov1metDI-12"
   },
   "outputs": [],
   "source": [
    "# Calculate key business metrics\n",
    "churn_rate = df['churned'].mean() * 100\n",
    "avg_monthly_revenue = df['monthly_revenue'].mean()\n",
    "total_mrr = df['monthly_revenue'].sum()\n",
    "campaign_coverage = df['retention_campaign_received'].mean() * 100\n",
    "\n",
    "print(\"Key Business Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Overall Churn Rate: {churn_rate:.2f}%\")\n",
    "print(f\"Average Monthly Revenue per Customer: ${avg_monthly_revenue:.2f}\")\n",
    "print(f\"Total Monthly Recurring Revenue (MRR): ${total_mrr:,.2f}\")\n",
    "print(f\"Retention Campaign Coverage: {campaign_coverage:.2f}%\")\n",
    "print(f\"\\nChurn by Plan Type:\")\n",
    "print(df.groupby('plan_type')['churned'].mean().sort_values(ascending=False) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9ZcADWbI-12"
   },
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeXA_srtI-13"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(\"=\"*60)\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_data, 'Percentage': missing_percent})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZcCn5PII-13"
   },
   "source": [
    "## Exploratory Data Analysis: Visualizations\n",
    "\n",
    "The following visualizations will help you understand customer behavior patterns. Save these charts for your business report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pSWAKG5I-13"
   },
   "outputs": [],
   "source": [
    "# Visualization 1: Churn Rate by Customer Segments\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Churn by Plan Type\n",
    "churn_by_plan = df.groupby('plan_type')['churned'].mean().sort_values(ascending=False) * 100\n",
    "axes[0, 0].bar(churn_by_plan.index, churn_by_plan.values, color='coral')\n",
    "axes[0, 0].set_title('Churn Rate by Plan Type', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Churn Rate (%)')\n",
    "axes[0, 0].set_xlabel('Plan Type')\n",
    "\n",
    "# Churn by Company Size\n",
    "churn_by_size = df.groupby('company_size')['churned'].mean().sort_values(ascending=False) * 100\n",
    "axes[0, 1].bar(churn_by_size.index, churn_by_size.values, color='skyblue')\n",
    "axes[0, 1].set_title('Churn Rate by Company Size', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Churn Rate (%)')\n",
    "axes[0, 1].set_xlabel('Company Size')\n",
    "\n",
    "# Churn by Industry\n",
    "churn_by_industry = df.groupby('industry')['churned'].mean().sort_values(ascending=False) * 100\n",
    "axes[1, 0].bar(churn_by_industry.index, churn_by_industry.values, color='lightgreen')\n",
    "axes[1, 0].set_title('Churn Rate by Industry', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Churn Rate (%)')\n",
    "axes[1, 0].set_xlabel('Industry')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Churn by Contract Length\n",
    "churn_by_contract = df.groupby('contract_length')['churned'].mean().sort_values(ascending=False) * 100\n",
    "axes[1, 1].bar(churn_by_contract.index, churn_by_contract.values, color='plum')\n",
    "axes[1, 1].set_title('Churn Rate by Contract Length', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Churn Rate (%)')\n",
    "axes[1, 1].set_xlabel('Contract Length')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('churn_by_segments.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: churn_by_segments.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKU_Ma3pI-13"
   },
   "outputs": [],
   "source": [
    "# Visualization 2: Revenue Distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Monthly Revenue Distribution\n",
    "axes[0].hist(df[df['monthly_revenue'] > 0]['monthly_revenue'], bins=50, color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Distribution of Monthly Revenue', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Monthly Revenue ($)')\n",
    "axes[0].set_ylabel('Number of Customers')\n",
    "axes[0].axvline(df['monthly_revenue'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${df[\"monthly_revenue\"].mean():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Monthly Revenue by Churn Status\n",
    "churned_revenue = df[df['churned'] == 1]['monthly_revenue']\n",
    "retained_revenue = df[df['churned'] == 0]['monthly_revenue']\n",
    "axes[1].hist([churned_revenue, retained_revenue], bins=30, label=['Churned', 'Retained'], color=['red', 'green'], alpha=0.7)\n",
    "axes[1].set_title('Monthly Revenue: Churned vs Retained', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Monthly Revenue ($)')\n",
    "axes[1].set_ylabel('Number of Customers')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('revenue_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: revenue_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EATBpR_eI-13"
   },
   "outputs": [],
   "source": [
    "# Visualization 3: Engagement Metrics by Churn Status\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Monthly Active Days\n",
    "df.boxplot(column='monthly_active_days', by='churned', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Monthly Active Days by Churn Status', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Churned (0=No, 1=Yes)')\n",
    "axes[0, 0].set_ylabel('Monthly Active Days')\n",
    "axes[0, 0].get_figure().suptitle('')\n",
    "\n",
    "# Customer Health Score\n",
    "df.boxplot(column='customer_health_score', by='churned', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Customer Health Score by Churn Status', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Churned (0=No, 1=Yes)')\n",
    "axes[0, 1].set_ylabel('Health Score')\n",
    "\n",
    "# Days Since Last Login\n",
    "df.boxplot(column='days_since_last_login', by='churned', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Days Since Last Login by Churn Status', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Churned (0=No, 1=Yes)')\n",
    "axes[1, 0].set_ylabel('Days Since Last Login')\n",
    "\n",
    "# Feature Adoption Score\n",
    "df.boxplot(column='feature_adoption_score', by='churned', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Feature Adoption Score by Churn Status', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Churned (0=No, 1=Yes)')\n",
    "axes[1, 1].set_ylabel('Feature Adoption Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('engagement_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: engagement_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3A7EcMVI-13"
   },
   "outputs": [],
   "source": [
    "# Visualization 4: Correlation Heatmap of Key Metrics\n",
    "# Select numerical features for correlation analysis\n",
    "numerical_features = ['monthly_revenue', 'monthly_active_days', 'feature_adoption_score',\n",
    "                     'support_tickets_count', 'login_frequency', 'session_duration_avg',\n",
    "                     'days_since_last_login', 'customer_health_score', 'nps_score', 'churned']\n",
    "\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap of Customer Metrics', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: correlation_heatmap.png\")\n",
    "print(\"\\nKey Correlations with Churn:\")\n",
    "print(correlation_matrix['churned'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvQDFnsiI-13"
   },
   "source": [
    "## Data Preparation for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5r4MbMOnI-13"
   },
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical columns, fill with median\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "print(\"Missing values handled\")\n",
    "print(f\"Remaining missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZEpFaxdI-13"
   },
   "outputs": [],
   "source": [
    "# Encode categorical variables for machine learning\n",
    "# Create label encoders for categorical columns\n",
    "categorical_cols = ['company_size', 'industry', 'country', 'plan_type', 'contract_length', 'payment_method']\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Categorical variables encoded\")\n",
    "print(f\"\\nNew encoded columns: {[f'{col}_encoded' for col in categorical_cols]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgJlgTB6I-13"
   },
   "source": [
    "---\n",
    "\n",
    "# Section 1B: Predictive Modeling\n",
    "\n",
    "In this section, we will build two predictive models:\n",
    "1. LightGBM for customer churn prediction\n",
    "2. Temporal Fusion Transformer for revenue forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNca8Ka4I-14"
   },
   "source": [
    "## Part 1: Customer Churn Prediction with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6g1WjkuI-14"
   },
   "outputs": [],
   "source": [
    "# Prepare features for churn prediction\n",
    "feature_columns = [\n",
    "    'company_size_encoded', 'industry_encoded', 'country_encoded',\n",
    "    'plan_type_encoded', 'contract_length_encoded', 'payment_method_encoded',\n",
    "    'monthly_revenue', 'payment_failures', 'discount_received',\n",
    "    'monthly_active_days', 'feature_adoption_score', 'support_tickets_count',\n",
    "    'login_frequency', 'session_duration_avg', 'api_calls_count',\n",
    "    'days_since_last_login', 'feature_usage_decline', 'customer_health_score',\n",
    "    'nps_score', 'product_feedback_submitted'\n",
    "]\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df['churned']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train):,} customers\")\n",
    "print(f\"Testing set size: {len(X_test):,} customers\")\n",
    "print(f\"\\nChurn rate in training set: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Churn rate in testing set: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atUYjpx4I-14"
   },
   "outputs": [],
   "source": [
    "# Train LightGBM model\n",
    "print(\"Training LightGBM model...\")\n",
    "print(\"This may take 1-2 minutes\")\n",
    "\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=20,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nModel training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thKKiByWI-14"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "y_pred_proba = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"LightGBM Churn Prediction Model Performance\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"Recall: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9D9sKRxeI-14"
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Retained', 'Churned'],\n",
    "            yticklabels=['Retained', 'Churned'])\n",
    "plt.title('Confusion Matrix: Churn Prediction Model', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUeaq9KYI-14"
   },
   "source": [
    "## Part 2: Revenue Forecasting with Temporal Fusion Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-1XX1BeI-14"
   },
   "outputs": [],
   "source": [
    "# Load time series data\n",
    "url = \"https://raw.githubusercontent.com/clarkian-teachings/data5k/main/assessments/revenue_timeseries.csv\"\n",
    "ts_df = pd.read_csv(url)\n",
    "ts_df['date'] = pd.to_datetime(ts_df['date'])\n",
    "\n",
    "print(f\"Time series data loaded: {len(ts_df):,} records\")\n",
    "print(f\"Number of unique customers: {ts_df['customer_id'].nunique():,}\")\n",
    "print(f\"Time range: {ts_df['date'].min()} to {ts_df['date'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee2XXkPsI-14"
   },
   "outputs": [],
   "source": [
    "# Encode categorical variables for time series\n",
    "for col in ['plan_type', 'company_size', 'customer_id']:\n",
    "    le = LabelEncoder()\n",
    "    ts_df[f'{col}_encoded'] = le.fit_transform(ts_df[col])\n",
    "\n",
    "print(\"Time series data prepared for forecasting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiSs_Q4mdewF"
   },
   "source": [
    "### Prepare data for Temporal Fusion Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQtrkCuyI-14"
   },
   "outputs": [],
   "source": [
    "print(\"Preparing Temporal Fusion Transformer model...\")\n",
    "\n",
    "max_prediction_length = 6  # Forecast 6 months ahead\n",
    "max_encoder_length = 12    # Use 12 months of history\n",
    "\n",
    "training_cutoff = ts_df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "# Convert encoded categorical columns to string type\n",
    "ts_df['plan_type_encoded'] = ts_df['plan_type_encoded'].astype(str)\n",
    "ts_df['company_size_encoded'] = ts_df['company_size_encoded'].astype(str)\n",
    "ts_df['customer_id_encoded'] = ts_df['customer_id_encoded'].astype(str)\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    ts_df[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"revenue\",\n",
    "    group_ids=[\"customer_id_encoded\"],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"plan_type_encoded\", \"company_size_encoded\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\"revenue\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, ts_df, predict=True, stop_randomization=True)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "print(\"\\nData prepared for TFT model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETrYflNTdnwV"
   },
   "source": [
    "### Train the TFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nm19O3joI-14"
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "# Train Temporal Fusion Transformer\n",
    "print(\"Training Temporal Fusion Transformer...\")\n",
    "print(\"This will take 5-10 minutes\")\n",
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\", default_hp_metric=False)  # logging results to a tensorboard\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=RMSE(),\n",
    "    log_interval=10,\n",
    "    optimizer=\"Adam\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best_tft_model\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],  # Add the checkpoint_callback here\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n",
    "print(\"\\nTFT model training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hwTZjt5dyG1"
   },
   "source": [
    "### Evaluate and Visualize the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RgTnXjkI-14"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path if hasattr(trainer, 'checkpoint_callback') else None  \n",
    "if best_model_path:\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)    \n",
    "else:\n",
    "    best_tft = tft\n",
    "\n",
    "predictions = best_tft.predict(\n",
    "    val_dataloader,\n",
    "    return_x=True,\n",
    "    trainer_kwargs=dict(accelerator=\"cpu\")\n",
    ")\n",
    "\n",
    "print(\"Revenue forecasts generated\")\n",
    "print(f\"Predictions shape: {predictions.output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUj8UtqGI-15"
   },
   "outputs": [],
   "source": [
    "# Visualize sample forecasts\n",
    "# Select first 4 customers for visualization\n",
    "raw_predictions = best_tft.predict(\n",
    "    val_dataloader,\n",
    "    mode=\"raw\",\n",
    "    return_x=True,\n",
    "    trainer_kwargs=dict(accelerator=\"cpu\")  # ← ADD THIS\n",
    ")\n",
    "\n",
    "for idx in range(min(4, len(raw_predictions.x['decoder_target']))):\n",
    "    best_tft.plot_prediction(raw_predictions.x, raw_predictions.output, idx=idx, add_loss_to_title=True)\n",
    "    plt.savefig(f'tft_forecast_customer_{idx+1}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Forecast charts saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTKtTNQJI-15"
   },
   "outputs": [],
   "source": [
    "# Calculate forecast accuracy metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Get actual and predicted values\n",
    "actuals = predictions.x['decoder_target']\n",
    "forecasts = predictions.output.squeeze()\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(actuals.flatten(), forecasts.flatten())\n",
    "rmse = np.sqrt(mean_squared_error(actuals.flatten(), forecasts.flatten()))\n",
    "# mape = np.mean(np.abs((actuals.flatten() - forecasts.flatten()) / actuals.flatten())) * 100\n",
    "\n",
    "print(\"Revenue Forecasting Model Performance\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Absolute Error (MAE): ${mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mX_B77EMI-15"
   },
   "source": [
    "---\n",
    "\n",
    "# Section 1C: Explainable AI Analysis with SHAP\n",
    "\n",
    "In this section, we use SHAP (SHapley Additive exPlanations) to understand which features are driving customer churn predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYfd-AfsI-15"
   },
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer\n",
    "print(\"Generating SHAP values...\")\n",
    "print(\"This may take 2-3 minutes\")\n",
    "\n",
    "explainer = shap.Explainer(lgbm_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Handle case where SHAP returns a list (for binary classification)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values_class1 = shap_values[1]  # SHAP values for churn class\n",
    "else:\n",
    "    shap_values_class1 = shap_values\n",
    "\n",
    "print(\"SHAP values calculated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0XNFYfjI-15"
   },
   "outputs": [],
   "source": [
    "# SHAP Summary Plot: Global Feature Importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values_class1, X_test, plot_type=\"bar\", show=False)\n",
    "plt.title('Feature Importance for Churn Prediction (SHAP)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: shap_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkJZarU6I-15"
   },
   "outputs": [],
   "source": [
    "# SHAP Summary Plot: Detailed View\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values_class1, X_test, show=False)\n",
    "plt.title('SHAP Summary Plot: Impact of Features on Churn', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_summary_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: shap_summary_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HohbSHy3I-15"
   },
   "outputs": [],
   "source": [
    "# Identify Top 5 Features\n",
    "feature_importance = np.abs(shap_values_class1).mean(axis=0)\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_test.columns,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 5 Features Driving Customer Churn\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in feature_importance_df.head(5).iterrows():\n",
    "    print(f\"{idx+1}. {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\nFull Feature Importance Ranking:\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zCDoOkKI-2H"
   },
   "outputs": [],
   "source": [
    "# SHAP Waterfall Plot: Individual Customer Explanation\n",
    "# Select a churned customer example\n",
    "churned_indices = X_test.index[y_test == 1].tolist()\n",
    "sample_idx = churned_indices[0] if churned_indices else 0\n",
    "sample_position = X_test.index.get_loc(sample_idx)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.waterfall_plot(shap.Explanation(values=shap_values_class1[sample_position],\n",
    "                                     base_values=explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,\n",
    "                                     data=X_test.iloc[sample_position],\n",
    "                                     feature_names=X_test.columns.tolist()))\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_waterfall.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: shap_waterfall.png\")\n",
    "print(\"\\nThis chart shows how each feature contributes to the churn prediction for one customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xMB_af0I-2H"
   },
   "outputs": [],
   "source": [
    "# SHAP Force Plot: Another Individual Customer\n",
    "plt.figure(figsize=(20, 3))\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,\n",
    "    shap_values_class1[sample_position],\n",
    "    X_test.iloc[sample_position],\n",
    "    matplotlib=True,\n",
    "    show=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_force_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: shap_force_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLJh9HNzI-2I"
   },
   "outputs": [],
   "source": [
    "# SHAP Dependence Plots for Top 3 Features\n",
    "top_features = feature_importance_df.head(3)['Feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    shap.dependence_plot(feature, shap_values_class1, X_test, ax=axes[idx], show=False)\n",
    "    axes[idx].set_title(f'SHAP Dependence: {feature}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_dependence_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: shap_dependence_plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylEsTDCAI-2I"
   },
   "source": [
    "---\n",
    "\n",
    "# Section 1D: Causal Analysis with EconML\n",
    "\n",
    "In this section, we use causal inference to evaluate the effectiveness of retention campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5A7xn-CqI-2I"
   },
   "outputs": [],
   "source": [
    "# Prepare data for causal analysis\n",
    "# Treatment: retention_campaign_received\n",
    "# Outcome: churned (we want to see if campaign reduces churn)\n",
    "# Controls: customer characteristics\n",
    "\n",
    "# Select features for causal model (exclude treatment and outcome)\n",
    "causal_features = [\n",
    "    'company_size_encoded', 'industry_encoded', 'plan_type_encoded',\n",
    "    'monthly_revenue', 'monthly_active_days', 'feature_adoption_score',\n",
    "    'customer_health_score', 'days_since_last_login'\n",
    "]\n",
    "\n",
    "X_causal = df[causal_features]\n",
    "T_causal = df['retention_campaign_received']  # Treatment\n",
    "Y_causal = df['churned']  # Outcome\n",
    "\n",
    "print(\"Causal Analysis Data Prepared\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of customers: {len(X_causal):,}\")\n",
    "print(f\"Treated customers (received campaign): {T_causal.sum():,} ({T_causal.mean()*100:.1f}%)\")\n",
    "print(f\"Control customers (no campaign): {(1-T_causal).sum():,} ({(1-T_causal.mean())*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpYMomcuI-2I"
   },
   "outputs": [],
   "source": [
    "# Naive comparison (before causal analysis)\n",
    "churn_with_campaign = df[df['retention_campaign_received'] == 1]['churned'].mean()\n",
    "churn_without_campaign = df[df['retention_campaign_received'] == 0]['churned'].mean()\n",
    "\n",
    "naive_effect = churn_without_campaign - churn_with_campaign\n",
    "\n",
    "print(\"Naive Comparison (Without Causal Adjustment)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Churn rate WITH campaign: {churn_with_campaign*100:.2f}%\")\n",
    "print(f\"Churn rate WITHOUT campaign: {churn_without_campaign*100:.2f}%\")\n",
    "print(f\"Naive effect: {naive_effect*100:.2f} percentage points\")\n",
    "print(\"\\nNote: This naive comparison does not account for selection bias.\")\n",
    "print(\"At-risk customers are more likely to receive the campaign.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgsDRR1dI-2I"
   },
   "outputs": [],
   "source": [
    "# Train Doubly Robust Learner for causal inference\n",
    "print(\"Training Causal Inference Model (Doubly Robust Learner)...\")\n",
    "print(\"This may take 2-3 minutes\")\n",
    "\n",
    "# Initialize the DRLearner\n",
    "dml = DRLearner(\n",
    "    model_propensity=RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    model_regression=RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    model_final=StatsModelsLinearRegression(),\n",
    "    cv=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "dml.fit(Y=Y_causal, T=T_causal, X=X_causal)\n",
    "\n",
    "print(\"\\nCausal model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PctBdf7GI-2I"
   },
   "outputs": [],
   "source": [
    "# Calculate Average Treatment Effect (ATE)\n",
    "ate = dml.ate(X=X_causal)\n",
    "ate_interval = dml.ate_interval(X=X_causal, alpha=0.05)\n",
    "\n",
    "print(\"Average Treatment Effect (ATE) Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ATE: {ate:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{ate_interval[0]:.4f}, {ate_interval[1]:.4f}]\")\n",
    "print(\"\\nInterpretation:\")\n",
    "if ate < 0:\n",
    "    print(f\"- The retention campaign REDUCES churn by {abs(ate)*100:.2f} percentage points on average\")\n",
    "    print(f\"- This means the campaign is EFFECTIVE\")\n",
    "else:\n",
    "    print(f\"- The retention campaign INCREASES churn by {ate*100:.2f} percentage points on average\")\n",
    "    print(f\"- This suggests the campaign may not be effective\")\n",
    "\n",
    "if ate_interval[0] < 0 < ate_interval[1]:\n",
    "    print(f\"- However, the confidence interval includes zero, so the effect may not be statistically significant\")\n",
    "else:\n",
    "    print(f\"- The effect is statistically significant at the 95% confidence level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsDbFFvzI-2I"
   },
   "outputs": [],
   "source": [
    "# Calculate Conditional Average Treatment Effect (CATE)\n",
    "# This shows how treatment effect varies by customer segment\n",
    "cate = dml.effect(X=X_causal)\n",
    "\n",
    "# Add CATE to dataframe\n",
    "df['cate'] = cate\n",
    "\n",
    "print(\"Conditional Average Treatment Effect (CATE) Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CATE Statistics:\")\n",
    "print(f\"Mean CATE: {cate.mean():.4f}\")\n",
    "print(f\"Median CATE: {np.median(cate):.4f}\")\n",
    "print(f\"Min CATE: {cate.min():.4f}\")\n",
    "print(f\"Max CATE: {cate.max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvHDFNBMI-2I"
   },
   "outputs": [],
   "source": [
    "# CATE Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(cate, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(cate.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean CATE: {cate.mean():.4f}')\n",
    "plt.axvline(0, color='black', linestyle='-', linewidth=1, label='No Effect')\n",
    "plt.xlabel('CATE (Treatment Effect)', fontsize=12)\n",
    "plt.ylabel('Number of Customers', fontsize=12)\n",
    "plt.title('Distribution of Conditional Average Treatment Effects', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('cate_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: cate_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pE-y23eUI-2I"
   },
   "outputs": [],
   "source": [
    "# CATE by Customer Segments\n",
    "# Analyze treatment effect by different customer characteristics\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# CATE by Plan Type\n",
    "cate_by_plan = df.groupby('plan_type')['cate'].mean().sort_values()\n",
    "axes[0, 0].barh(cate_by_plan.index, cate_by_plan.values, color='skyblue')\n",
    "axes[0, 0].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[0, 0].set_xlabel('Average CATE')\n",
    "axes[0, 0].set_title('Treatment Effect by Plan Type', fontweight='bold')\n",
    "\n",
    "# CATE by Company Size\n",
    "cate_by_size = df.groupby('company_size')['cate'].mean().sort_values()\n",
    "axes[0, 1].barh(cate_by_size.index, cate_by_size.values, color='lightcoral')\n",
    "axes[0, 1].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[0, 1].set_xlabel('Average CATE')\n",
    "axes[0, 1].set_title('Treatment Effect by Company Size', fontweight='bold')\n",
    "\n",
    "# CATE by Revenue Segment\n",
    "df['revenue_segment'] = pd.cut(df['monthly_revenue'], bins=[0, 50, 150, 500, 2000],\n",
    "                                labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "cate_by_revenue = df.groupby('revenue_segment')['cate'].mean().sort_values()\n",
    "axes[1, 0].barh(cate_by_revenue.index, cate_by_revenue.values, color='lightgreen')\n",
    "axes[1, 0].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1, 0].set_xlabel('Average CATE')\n",
    "axes[1, 0].set_title('Treatment Effect by Revenue Segment', fontweight='bold')\n",
    "\n",
    "# CATE by Health Score Segment\n",
    "df['health_segment'] = pd.cut(df['customer_health_score'], bins=[0, 40, 60, 80, 100],\n",
    "                               labels=['Poor', 'Fair', 'Good', 'Excellent'])\n",
    "cate_by_health = df.groupby('health_segment')['cate'].mean().sort_values()\n",
    "axes[1, 1].barh(cate_by_health.index, cate_by_health.values, color='plum')\n",
    "axes[1, 1].axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1, 1].set_xlabel('Average CATE')\n",
    "axes[1, 1].set_title('Treatment Effect by Health Score', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cate_by_segments.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as: cate_by_segments.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvLrkh0qI-2I"
   },
   "source": [
    "---\n",
    "\n",
    "# Section 2: Summary and Report Guidance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAiSx66PI-2I"
   },
   "source": [
    "## Key Findings Checklist\n",
    "\n",
    "Use this checklist to ensure you capture all important insights for your report:\n",
    "\n",
    "### Section 1: Business Problem and Data Overview\n",
    "- [ ] Overall churn rate and customer demographics\n",
    "- [ ] Key patterns in customer behavior\n",
    "- [ ] Data quality observations\n",
    "\n",
    "### Section 2: Predictive Analytics Findings\n",
    "- [ ] LightGBM model performance metrics (accuracy, precision, recall, F1, ROC-AUC)\n",
    "- [ ] Confusion matrix interpretation\n",
    "- [ ] Revenue forecasting accuracy (MAE, RMSE, MAPE)\n",
    "- [ ] Sample forecast visualizations\n",
    "\n",
    "### Section 3: Explainable AI Insights\n",
    "- [ ] Top 5 features driving churn (from SHAP analysis)\n",
    "- [ ] How different customer segments behave differently\n",
    "- [ ] Feature importance rankings\n",
    "\n",
    "### Section 4: Causal Analysis\n",
    "- [ ] Average Treatment Effect (ATE) of retention campaigns\n",
    "- [ ] Which customer segments benefit most from campaigns (CATE)\n",
    "- [ ] Recommendations for targeted interventions\n",
    "\n",
    "### Section 5: Recommendations\n",
    "- [ ] Data-driven retention strategy recommendations\n",
    "- [ ] Customer segmentation for targeted campaigns\n",
    "- [ ] Resource allocation suggestions\n",
    "- [ ] Expected ROI and business impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22XwnplxI-2I"
   },
   "source": [
    "## Writing Your Business Report\n",
    "\n",
    "Remember to:\n",
    "\n",
    "1. **Write for a business audience**: Avoid technical jargon. Explain findings in terms of business impact.\n",
    "\n",
    "2. **Use the outputs above**: Reference specific numbers, charts, and findings from this notebook.\n",
    "\n",
    "3. **Connect insights to actions**: Every finding should lead to a recommendation.\n",
    "\n",
    "4. **Focus on ROI**: Quantify the business impact where possible (e.g., revenue at risk, cost savings).\n",
    "\n",
    "5. **Be concise**: Maximum 1,200 words. Every sentence should add value.\n",
    "\n",
    "6. **Professional formatting**: Use headers, bullet points, and charts effectively.\n",
    "\n",
    "7. **Executive summary**: Start with a clear, compelling summary of your key findings and recommendations.\n",
    "\n",
    "Good luck with your report!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReaMaIFCI-2I"
   },
   "source": [
    "---\n",
    "\n",
    "## Additional Notes\n",
    "\n",
    "- All code cells have been executed and outputs generated\n",
    "- Charts have been saved to your working directory\n",
    "- You can re-run any cell to regenerate outputs\n",
    "- For questions about the analysis, consult the course materials or your instructor\n",
    "\n",
    "**Submission Reminder:**\n",
    "- Submit your business report (.docx format) via Turnitin by Tuesday (23:55 AEST), Week 5\n",
    "\n",
    "- Ensure your report is maximum 1,200 words (excluding charts and tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.48.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shap\n",
    "shap.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu121'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_forecasting\n",
    "pytorch_forecasting.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "lgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import econml\n",
    "econml.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
