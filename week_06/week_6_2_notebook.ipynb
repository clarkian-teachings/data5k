{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU87uDXgjPzu"
      },
      "source": [
        "## 1. Download a Sample Video\n",
        "\n",
        "First, let's download a sample traffic video and trim it to 10 seconds to make processing faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QvLdy-LjPzv"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install wget moviepy ultralytics opencv-python -q\n",
        "\n",
        "# Download a sample traffic video from Pexels\n",
        "!wget \"https://videos.pexels.com/video-files/2103099/2103099-uhd_2560_1440_30fps.mp4\" -O original_traffic_video.mp4\n",
        "\n",
        "# Check if the original video file exists and show its size\n",
        "!ls -lh original_traffic_video.mp4\n",
        "\n",
        "# Trim the video to 10 seconds to make processing faster\n",
        "from moviepy.editor import VideoFileClip\n",
        "print(\"Trimming video to first 10 seconds...\")\n",
        "\n",
        "# Load the video and create a 10-second clip\n",
        "original_clip = VideoFileClip(\"original_traffic_video.mp4\")\n",
        "trimmed_clip = original_clip.subclip(0, 10)  # Extract first 10 seconds\n",
        "\n",
        "# Save the trimmed video\n",
        "trimmed_clip.write_videofile(\"traffic_video.mp4\", codec=\"libx264\", audio=False)\n",
        "original_clip.close()\n",
        "trimmed_clip.close()\n",
        "\n",
        "# Check the size of the trimmed video\n",
        "!ls -lh traffic_video.mp4\n",
        "print(\"Video successfully trimmed to 10 seconds!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP8PVfd8jPzy"
      },
      "source": [
        "## 2. Import Libraries and Set Up YOLOv8\n",
        "\n",
        "Let's import the necessary libraries for our project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "489vJjtRjPzy"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xow7bRzmjPzz"
      },
      "source": [
        "## 3. Load a Pre-trained YOLOv8 Model\n",
        "\n",
        "Now, let's download and load a pre-trained YOLOv8 model. We'll use the YOLOv8n model, which is relatively small and fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5_G5p6rjPzz"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained YOLOv8 model\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Display model info\n",
        "print(f\"Model loaded: {model}\")\n",
        "print(f\"Model task: {model.task}\")\n",
        "\n",
        "# Classes that YOLOv8 can detect\n",
        "print(\"\\nYOLOv8 can detect these classes:\")\n",
        "for i, class_name in enumerate(model.names.values()):\n",
        "    print(f\"{i}: {class_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILg3Ub6IjPz0"
      },
      "source": [
        "Find the class IDs for vehicles in the list above. Typically, these are:\n",
        "- 2: Car\n",
        "- 5: Bus\n",
        "- 7: Truck\n",
        "\n",
        "We'll focus on these classes for our car detection task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt2w1WSOjPz1"
      },
      "source": [
        "## 4. Detect Cars in One Frame\n",
        "\n",
        "Let's extract a single frame from the video and detect cars in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI5ZrJ2YjPz1"
      },
      "outputs": [],
      "source": [
        "# Open the video file\n",
        "cap = cv2.VideoCapture('traffic_video.mp4')\n",
        "\n",
        "# Check if the video file opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video file\")\n",
        "else:\n",
        "    # Read the first frame\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if ret:\n",
        "        # Get video details\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        print(f\"Video details:\")\n",
        "        print(f\"Resolution: {frame_width}x{frame_height}\")\n",
        "        print(f\"FPS: {fps}\")\n",
        "        print(f\"Total frames: {total_frames}\")\n",
        "        print(f\"Duration: {total_frames/fps:.2f} seconds\")\n",
        "\n",
        "        # Save the first frame as an image\n",
        "        cv2.imwrite('first_frame.jpg', frame)\n",
        "        print(\"\\nFirst frame saved as 'first_frame.jpg'\")\n",
        "\n",
        "        # Display the frame\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(\"First Frame of the Video\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Error: Could not read the first frame\")\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO11knO6jPz2"
      },
      "source": [
        "Now let's use YOLOv8 to detect vehicles in the first frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQi6KEOejPz2"
      },
      "outputs": [],
      "source": [
        "# Load the saved first frame\n",
        "frame = cv2.imread('first_frame.jpg')\n",
        "\n",
        "# Define the classes we're interested in (car, bus, truck)\n",
        "vehicle_classes = [2, 5, 7]  # COCO dataset class IDs for car, bus, truck\n",
        "\n",
        "# Run YOLOv8 inference on the frame\n",
        "results = model(frame)\n",
        "\n",
        "# Get detection results from the first detection (there's only one image)\n",
        "result = results[0]\n",
        "\n",
        "# Initialize a list to store vehicle detections\n",
        "vehicle_detections = []\n",
        "\n",
        "# Filter for vehicle classes and confidence threshold\n",
        "for box in result.boxes:\n",
        "    class_id = int(box.cls.item())\n",
        "    confidence = box.conf.item()\n",
        "\n",
        "    # Only include vehicles with confidence > 0.5\n",
        "    if class_id in vehicle_classes and confidence > 0.5:\n",
        "        # Get bounding box coordinates (convert from PyTorch tensor to integers)\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "\n",
        "        # Add to our vehicle detections list\n",
        "        class_name = model.names[class_id]\n",
        "        vehicle_detections.append((x1, y1, x2, y2, confidence, class_name))\n",
        "\n",
        "# Create a copy of the frame to draw on\n",
        "annotated_frame = frame.copy()\n",
        "\n",
        "# Draw bounding boxes for vehicle detections\n",
        "for x1, y1, x2, y2, conf, class_name in vehicle_detections:\n",
        "    # Draw rectangle\n",
        "    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    # Prepare label text\n",
        "    label = f\"{class_name}: {conf:.2f}\"\n",
        "\n",
        "    # Determine text size and position\n",
        "    (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
        "    y1 = max(y1, text_height + 5)\n",
        "\n",
        "    # Draw filled rectangle for text background\n",
        "    cv2.rectangle(annotated_frame, (x1, y1 - text_height - 5), (x1 + text_width, y1), (0, 255, 0), -1)\n",
        "\n",
        "    # Add text\n",
        "    cv2.putText(annotated_frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
        "\n",
        "# Add a total count to the image\n",
        "total_count = len(vehicle_detections)\n",
        "count_text = f\"Total Vehicles: {total_count}\"\n",
        "cv2.putText(annotated_frame, count_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "# Display the annotated frame\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Vehicle Detection on First Frame\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Save the annotated frame\n",
        "cv2.imwrite('annotated_first_frame.jpg', annotated_frame)\n",
        "print(f\"Detected {total_count} vehicles\")\n",
        "print(\"\\nAnnotated frame saved as 'annotated_first_frame.jpg'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ic0lIsjPz3"
      },
      "source": [
        "## 5 & 6. Process the Entire Video and Show Results\n",
        "\n",
        "Now, let's process the entire video, detect vehicles in each frame, and create a new video with annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1p29F0ajPz4"
      },
      "outputs": [],
      "source": [
        "# Define a function to process the video\n",
        "def process_video(input_path, output_path, model, target_classes, conf_threshold=0.5):\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "    # Check if the video opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Create video writer for output\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    # Initialize a counter for processed frames\n",
        "    frame_count = 0\n",
        "\n",
        "    # Process frames\n",
        "    while cap.isOpened():\n",
        "        # Read a frame\n",
        "        success, frame = cap.read()\n",
        "\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        # Update frame count\n",
        "        frame_count += 1\n",
        "\n",
        "        # Print progress every 10 frames\n",
        "        if frame_count % 10 == 0:\n",
        "            print(f\"Processing frame {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%)\")\n",
        "\n",
        "        # Run YOLOv8 inference\n",
        "        results = model(frame)\n",
        "        result = results[0]\n",
        "\n",
        "        # Filter for vehicle classes\n",
        "        detections = []\n",
        "        for box in result.boxes:\n",
        "            class_id = int(box.cls.item())\n",
        "            confidence = box.conf.item()\n",
        "\n",
        "            if class_id in target_classes and confidence > conf_threshold:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "                class_name = model.names[class_id]\n",
        "                detections.append((x1, y1, x2, y2, confidence, class_name))\n",
        "\n",
        "        # Create a copy of the frame\n",
        "        annotated_frame = frame.copy()\n",
        "\n",
        "        # Draw bounding boxes\n",
        "        for x1, y1, x2, y2, conf, class_name in detections:\n",
        "            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Prepare label text\n",
        "            label = f\"{class_name}: {conf:.2f}\"\n",
        "\n",
        "            # Determine text size and position\n",
        "            (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
        "            y1 = max(y1, text_height + 5)\n",
        "\n",
        "            # Draw filled rectangle for text background\n",
        "            cv2.rectangle(annotated_frame, (x1, y1 - text_height - 5), (x1 + text_width, y1), (0, 255, 0), -1)\n",
        "\n",
        "            # Add text\n",
        "            cv2.putText(annotated_frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
        "\n",
        "        # Add a total count to the frame\n",
        "        total_count = len(detections)\n",
        "        count_text = f\"Total Vehicles: {total_count}\"\n",
        "        cv2.putText(annotated_frame, count_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "        # Add frame number\n",
        "        frame_text = f\"Frame: {frame_count}/{total_frames}\"\n",
        "        cv2.putText(annotated_frame, frame_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "        # Write the frame to the output video\n",
        "        out.write(annotated_frame)\n",
        "\n",
        "    # Release video capture and writer\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(f\"\\nProcessing complete! Output saved to {output_path}\")\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzqxL9_FjPz4"
      },
      "outputs": [],
      "source": [
        "# Define the target classes (vehicles)\n",
        "vehicle_classes = [2, 5, 7]  # car, bus, truck\n",
        "\n",
        "# Process the video (this will be much faster with the 10-second clip)\n",
        "output_path = process_video(\n",
        "    input_path='traffic_video.mp4',\n",
        "    output_path='annotated_traffic_video.mp4',\n",
        "    model=model,\n",
        "    target_classes=vehicle_classes,\n",
        "    conf_threshold=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49q2QtsNjPz5"
      },
      "source": [
        "## Display the Annotated Video\n",
        "\n",
        "Let's display the processed video with vehicle detections."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -y -i annotated_traffic_video.mp4 \\\n",
        "  -c:v libx264 -pix_fmt yuv420p -movflags +faststart \\\n",
        "  -crf 23 -preset veryfast \\\n",
        "  annotated_traffic_video_h264.mp4"
      ],
      "metadata": {
        "id": "GuDv2QAP_Gt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4JG5iKyjPz5"
      },
      "outputs": [],
      "source": [
        "# Function to display the video in the notebook\n",
        "def display_video(video_path):\n",
        "    # Read the video file\n",
        "    video_file = open(video_path, \"rb\")\n",
        "    video_bytes = video_file.read()\n",
        "    video_file.close()\n",
        "\n",
        "    # Convert to base64\n",
        "    encoded = b64encode(video_bytes).decode()\n",
        "\n",
        "    # Display the video using HTML\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=\"800\" height=\"600\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">\n",
        "        Your browser does not support the video tag.\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "# Display the annotated video\n",
        "print(\"Displaying the video with vehicle detections:\")\n",
        "display_video('annotated_traffic_video.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -y -i annotated_traffic_video.mp4 \\\n",
        "  -c:v libx264 -pix_fmt yuv420p -movflags +faststart \\\n",
        "  -crf 23 -preset veryfast \\\n",
        "  annotated_traffic_video_h264.mp4"
      ],
      "metadata": {
        "id": "GqejhivuJWSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -j annotated_traffic_video_h264.zip /content/annotated_traffic_video_h264.mp4\n",
        "from google.colab import files\n",
        "files.download(\"annotated_traffic_video_h264.zip\")"
      ],
      "metadata": {
        "id": "HY9AV2u0JU9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zhEcBuzjPz6"
      },
      "source": [
        "## Bonus Challenge: Implement a Vehicle Counter\n",
        "\n",
        "As a challenge for workshop participants, try to modify the code to:\n",
        "\n",
        "1. Count vehicles only when they cross a specific line in the video\n",
        "2. Track the same vehicle across multiple frames\n",
        "3. Classify vehicle types and count them separately\n",
        "4. Estimate vehicle speeds based on their movement between frames\n",
        "\n",
        "This will require additional research into OpenCV tracking methods or other tracking libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7UA4AL6jPz6"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this workshop, we've learned how to:\n",
        "1. Download a sample video and trim it to 10 seconds for faster processing\n",
        "2. Install YOLOv8 and necessary libraries\n",
        "3. Load a pre-trained model\n",
        "4. Detect vehicles in a single frame\n",
        "5. Process the entire video\n",
        "6. Display the results with vehicle labels and counts\n",
        "\n",
        "You can now adapt this code for your own projects, using different videos or detecting different objects!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK4opyMTjPz6"
      },
      "source": [
        "## Resources and Further Reading\n",
        "\n",
        "- [Ultralytics YOLOv8 Documentation](https://docs.ultralytics.com/)\n",
        "- [OpenCV Documentation](https://docs.opencv.org/)\n",
        "- [COCO Dataset Classes](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/)\n",
        "- [Object Tracking Tutorials](https://learnopencv.com/object-tracking-using-opencv-cpp-python/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1+2"
      ],
      "metadata": {
        "id": "ePQUxkTPFRc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-r-ORXp-9yna"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}